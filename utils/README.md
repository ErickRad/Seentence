# ğŸ–¼ï¸ Seentence

**Seentence** is a deep learning project for automatic image captioning using PyTorch. It takes an image as input and generates a descriptive sentence about its contents, combining computer vision (CNN) and natural language processing (Transformer).

---

ğŸ“š **Project Structure**

  ğŸ“‚ datasets              â†’ Custom PyTorch dataset logic
  
     â†’ğŸ“ dataset.py

  ğŸ“‚ models                â†’ Encoder-Decoder architecture

     â†’ğŸ§  captioningModel.py
    
     â†’ğŸ§© cnnEncoder.py
    
     â†’ğŸ“ transformerDecoder.py
    

  ğŸ“‚ utils                 â†’ Vocab, transforms, training tools
  
     â†’ğŸ”¤ vocab.py
    
     â†’ğŸ›ï¸ dataAugmentation.py
    
     â†’ğŸ› ï¸ trainUtils.py
     

  ğŸ“¥ getData.py            â†’ Download and unzip
  
  ğŸ‹ï¸ train.py             â†’ Train the model
  
  ğŸ“Š evaluate.py          â†’ Evaluate BLEU on test set
  
  ğŸ–¼ï¸ testImage.py         â†’ Generate caption for a single image
  
  âš™ï¸ config.yaml           â†’ Configs and hyperparameters
  
  ğŸ“˜ README.md             â†’ You are here!

---

âš™ï¸ **Requirements**

- Python 3.8+
- PyTorch
- torchvision
- pandas
- Pillow
- nltk
- pyyaml
- tqdm (opcional)

Install requirements:

pip install -r requirements.txt

---

ğŸ“¥ **Download the Dataset**

Run the script below to automatically download and extract the dataset:

python data/getData.py

It will:

- Download the image zip and annotations CSV from HuggingFace

- Extract images to data/images/

- Clean up the zip file

---

ğŸ”§ **Configuration**

Edit the config.yaml file to adjust:

-Paths (imagesPath, annotationsPath)

-Hyperparameters (batch size, learning rate, hidden size, etc.)

-Training settings (epochs, dropout, scheduler)

---

ğŸš€ **Training**

To train the model:

python train.py

- This will save the best model to the path specified in config.yaml under     checkpoints folder.

---

ğŸ“ˆ **Evaluation**

To evaluate your model on the test split (BLEU score):

python evaluate.py

---

ğŸ§ª **Test a Single Image**
You can run a custom image through the model to generate a caption (after training):

python testImage.py --image-path path/to/image.jpg

---

ğŸ§  **Model Architecture**
CNN Encoder: Extracts visual features from input image

Transformer Decoder: Generates caption token-by-token

Vocabulary: Handles tokenization, mapping, padding

--

ğŸ“ **Dataset**
I have used the Flickr30k dataset, which contains 30,000 images and 5 human-written captions per image. Normalized to 224x224 and sent to model to recongnize edges, formats and shadows. After that, ponderate wheights to words to contruct a sentence about the image.

---

ğŸ¤  **Contributing**
Feel free to open issues, suggest improvements, or fork the repo.